import requests
import re
from urllib.parse import urlparse, urljoin
import time

target_url = "abanchy.com"
target_links = []


def request(url):
    try:
        return requests.get("https://"+url)
    except requests.exceptions.ConnectionError:
        pass


def extract_links_from(url):

    response = request(url)

    # Find href anchor tags with a regex
    if response is not None:
        return re.findall('(?:href=")(.*?)"', response.content.decode('utf-8'))


def crawl(url):
    href_links = extract_links_from(url)
    if href_links is not None:
        print(href_links)
    if href_links and len(href_links) > 0:
        for link in href_links:
            link = urljoin(url, link)
            if target_url in link and link not in target_links:
                target_links.append(link)
                print("PAGE END >>> "+link)
                time.sleep(1)
                crawl(link)


crawl(target_url)



        